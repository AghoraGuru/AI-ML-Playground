{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:46:10.042724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 11:46:11.387066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/kalyan/DataSets/animals/train/train/\"\n",
    "val_path = \"/home/kalyan/DataSets/animals/test/test/\"\n",
    "train_df = pd.read_csv(\"/home/kalyan/DataSets/animals/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/kalyan/DataSets/animals/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "import math\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 30\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "    \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=train_path,\n",
    "    x_col=\"Image_id\",\n",
    "    y_col=\"Animal\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=validation_df,\n",
    "    directory=train_path,\n",
    "    x_col=\"Image_id\",\n",
    "    y_col=\"Animal\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(150, 150))\n",
    "\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 30:\n",
    "        lr *= 0.1\n",
    "    elif epoch > 20:\n",
    "        lr *= 0.5\n",
    "    print('Learning rate:', lr)\n",
    "    return lr\n",
    "\n",
    "# Define ResNet50 model with modified top layers\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=lr_schedule(0)),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=len(train_generator),\n",
    "                              epochs=EPOCHS,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=len(val_generator),\n",
    "                              callbacks=[lr_scheduler, early_stop])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/kalyan/DataSets/animals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'train.csv', 'trainLabels.npy', 'trainbeg.npy', 'test.csv', 'sample_submission.csv', 'testbeg.npy', 'self_vgg16.h5', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(path + \"trainbeg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(path + \"testbeg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 1)\n",
      "(13000,)\n",
      "[11 26 10 ... 26 26 26]\n"
     ]
    }
   ],
   "source": [
    "Y_train =  np.load(path + \"trainLabels.npy\")\n",
    "print(Y_train.shape)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0])\n",
    "np.squeeze(Y_train)\n",
    "print(Y_train.shape)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:15:51.071541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 10:15:51.705256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-08 10:15:53.901629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:53.920613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:53.920815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:53.922767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:53.923058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:53.923211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:54.485426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:54.485631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:54.485788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-08 10:15:54.485915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2579 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp_dm = X_train.shape[1]\n",
    "vgg_model_path = path + '/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(weights=vgg_model_path,include_top=False,input_shape=(inp_dm,inp_dm, 3))\n",
    "conv_base.trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        layer.trainable = True\n",
    "    if layer.name == 'block4_conv1':\n",
    "        layer.trainable = True    \n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import Dense\n",
    "\n",
    "def VGG16_classifier():    \n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 2, 2, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                15390     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,779,166\n",
      "Trainable params: 1,064,478\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:21:39.215870: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n",
      "2023-04-08 10:21:39.634430: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:21:41.130758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2023-04-08 10:21:43.301785: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x33347c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-08 10:21:43.301827: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-04-08 10:21:43.307899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-08 10:21:43.426714: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 15s 53ms/step - loss: 2.5755 - accuracy: 0.2758 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 2.1050 - accuracy: 0.3878 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 1.8954 - accuracy: 0.4429 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 1.7469 - accuracy: 0.4845 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 1.6274 - accuracy: 0.5157 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "204/204 [==============================] - 10s 48ms/step - loss: 1.5175 - accuracy: 0.5464 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 1.4035 - accuracy: 0.5821 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 1.3106 - accuracy: 0.6069 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 1.2202 - accuracy: 0.6322 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 1.1309 - accuracy: 0.6543 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 1.0417 - accuracy: 0.6869 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.9687 - accuracy: 0.7058 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.8945 - accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.8302 - accuracy: 0.7505 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.7749 - accuracy: 0.7692 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 0.7112 - accuracy: 0.7890 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.6578 - accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "204/204 [==============================] - 10s 50ms/step - loss: 0.6000 - accuracy: 0.8210 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.5589 - accuracy: 0.8355 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.5286 - accuracy: 0.8406 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.4732 - accuracy: 0.8612 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.4434 - accuracy: 0.8691 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.4065 - accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.3852 - accuracy: 0.8848 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.3554 - accuracy: 0.8968 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.3342 - accuracy: 0.9037 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.3191 - accuracy: 0.9051 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2885 - accuracy: 0.9175 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2780 - accuracy: 0.9180 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2646 - accuracy: 0.9242 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2540 - accuracy: 0.9267 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2248 - accuracy: 0.9363 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2057 - accuracy: 0.9438 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.2265 - accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1871 - accuracy: 0.9473 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1839 - accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1674 - accuracy: 0.9532 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1817 - accuracy: 0.9472 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1556 - accuracy: 0.9561 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1670 - accuracy: 0.9501 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1344 - accuracy: 0.9634 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1381 - accuracy: 0.9590 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.1344 - accuracy: 0.9604 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.1432 - accuracy: 0.9575 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.0737 - accuracy: 0.9842 - lr: 2.0000e-04\n",
      "Epoch 46/100\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.0553 - accuracy: 0.9892 - lr: 2.0000e-04\n",
      "Epoch 47/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0514 - accuracy: 0.9901 - lr: 2.0000e-04\n",
      "Epoch 48/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0476 - accuracy: 0.9905 - lr: 2.0000e-04\n",
      "Epoch 49/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0476 - accuracy: 0.9909 - lr: 2.0000e-04\n",
      "Epoch 50/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0469 - accuracy: 0.9915 - lr: 2.0000e-04\n",
      "Epoch 51/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0444 - accuracy: 0.9911 - lr: 2.0000e-04\n",
      "Epoch 52/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0461 - accuracy: 0.9915 - lr: 2.0000e-04\n",
      "Epoch 53/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0432 - accuracy: 0.9917 - lr: 2.0000e-04\n",
      "Epoch 54/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0437 - accuracy: 0.9928 - lr: 2.0000e-04\n",
      "Epoch 55/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0385 - accuracy: 0.9944 - lr: 2.0000e-04\n",
      "Epoch 56/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0401 - accuracy: 0.9927 - lr: 2.0000e-04\n",
      "Epoch 57/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0402 - accuracy: 0.9925 - lr: 2.0000e-04\n",
      "Epoch 58/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0372 - accuracy: 0.9935 - lr: 2.0000e-04\n",
      "Epoch 59/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0380 - accuracy: 0.9928 - lr: 2.0000e-04\n",
      "Epoch 60/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0361 - accuracy: 0.9947 - lr: 2.0000e-04\n",
      "Epoch 61/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0379 - accuracy: 0.9930 - lr: 2.0000e-04\n",
      "Epoch 62/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0349 - accuracy: 0.9945 - lr: 2.0000e-04\n",
      "Epoch 63/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0341 - accuracy: 0.9949 - lr: 2.0000e-04\n",
      "Epoch 64/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0347 - accuracy: 0.9931 - lr: 2.0000e-04\n",
      "Epoch 65/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0325 - accuracy: 0.9950 - lr: 2.0000e-04\n",
      "Epoch 66/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0314 - accuracy: 0.9955 - lr: 2.0000e-04\n",
      "Epoch 67/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0303 - accuracy: 0.9958 - lr: 2.0000e-04\n",
      "Epoch 68/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0298 - accuracy: 0.9952 - lr: 2.0000e-04\n",
      "Epoch 69/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0322 - accuracy: 0.9944 - lr: 2.0000e-04\n",
      "Epoch 70/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0296 - accuracy: 0.9956 - lr: 2.0000e-04\n",
      "Epoch 71/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0300 - accuracy: 0.9961 - lr: 2.0000e-04\n",
      "Epoch 72/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0289 - accuracy: 0.9952 - lr: 2.0000e-04\n",
      "Epoch 73/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0291 - accuracy: 0.9953 - lr: 2.0000e-04\n",
      "Epoch 74/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0286 - accuracy: 0.9961 - lr: 2.0000e-04\n",
      "Epoch 75/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0251 - accuracy: 0.9967 - lr: 2.0000e-04\n",
      "Epoch 76/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0282 - accuracy: 0.9952 - lr: 2.0000e-04\n",
      "Epoch 77/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0280 - accuracy: 0.9958 - lr: 2.0000e-04\n",
      "Epoch 78/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0260 - accuracy: 0.9949 - lr: 2.0000e-04\n",
      "Epoch 79/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0232 - accuracy: 0.9967 - lr: 4.0000e-05\n",
      "Epoch 80/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0185 - accuracy: 0.9978 - lr: 4.0000e-05\n",
      "Epoch 81/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0192 - accuracy: 0.9981 - lr: 4.0000e-05\n",
      "Epoch 82/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0198 - accuracy: 0.9977 - lr: 4.0000e-05\n",
      "Epoch 83/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0188 - accuracy: 0.9978 - lr: 4.0000e-05\n",
      "Epoch 84/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0187 - accuracy: 0.9975 - lr: 8.0000e-06\n",
      "Epoch 85/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0180 - accuracy: 0.9977 - lr: 8.0000e-06\n",
      "Epoch 86/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0180 - accuracy: 0.9976 - lr: 8.0000e-06\n",
      "Epoch 87/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0174 - accuracy: 0.9977 - lr: 8.0000e-06\n",
      "Epoch 88/100\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.0180 - accuracy: 0.9979 - lr: 8.0000e-06\n",
      "Epoch 89/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0173 - accuracy: 0.9978 - lr: 8.0000e-06\n",
      "Epoch 90/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0180 - accuracy: 0.9975 - lr: 8.0000e-06\n",
      "Epoch 91/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0177 - accuracy: 0.9974 - lr: 8.0000e-06\n",
      "Epoch 92/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0182 - accuracy: 0.9974 - lr: 8.0000e-06\n",
      "Epoch 93/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0181 - accuracy: 0.9978 - lr: 1.6000e-06\n",
      "Epoch 94/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0192 - accuracy: 0.9967 - lr: 1.6000e-06\n",
      "Epoch 95/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0180 - accuracy: 0.9978 - lr: 1.6000e-06\n",
      "Epoch 96/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0175 - accuracy: 0.9975 - lr: 3.2000e-07\n",
      "Epoch 97/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0184 - accuracy: 0.9978 - lr: 3.2000e-07\n",
      "Epoch 98/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0191 - accuracy: 0.9973 - lr: 3.2000e-07\n",
      "Epoch 99/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0162 - accuracy: 0.9985 - lr: 6.4000e-08\n",
      "Epoch 100/100\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.0180 - accuracy: 0.9978 - lr: 6.4000e-08\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import  ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "model1 = VGG16_classifier()\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3)]\n",
    "\n",
    "history_vgg = model1.fit(x = X_train/255.,y = to_categorical(Y_train, num_classes=30),batch_size=64,epochs=100,callbacks = callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(path + \"self_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:44:16.730940: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n",
      "2023-04-08 10:44:17.240159: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 11s 25ms/step\n",
      "0.9998461538461538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_train_predict = np.argmax(model1.predict(X_train/255.),axis=1)\n",
    "print(accuracy_score(Y_train,y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 5s 25ms/step\n",
      "[[1.49796442e-08 1.94970102e-08 1.63242646e-08 ... 3.26142690e-05\n",
      "  8.16342393e-12 2.67197939e-12]\n",
      " [5.12042631e-09 3.37313804e-06 1.42101007e-05 ... 6.39694557e-03\n",
      "  3.34410346e-04 1.75660261e-06]\n",
      " [9.92887437e-01 1.99221631e-05 1.50331267e-04 ... 4.59864680e-07\n",
      "  8.47705394e-07 5.19748486e-04]\n",
      " ...\n",
      " [8.19700432e-08 1.92425460e-01 5.47298882e-03 ... 1.72403473e-02\n",
      "  7.84731749e-03 6.47731051e-02]\n",
      " [3.77091638e-08 5.50205048e-10 2.93335400e-09 ... 2.63111175e-13\n",
      "  1.70984968e-08 1.47328765e-05]\n",
      " [2.30001600e-08 5.84512634e-07 4.77698428e-04 ... 1.14654652e-09\n",
      "  2.68664880e-04 2.85157676e-05]]\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = model1.predict(X_test/255.)\n",
    "print(y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Img-1.jpg\n",
      "1          Img-2.jpg\n",
      "2          Img-3.jpg\n",
      "3          Img-4.jpg\n",
      "4          Img-5.jpg\n",
      "            ...     \n",
      "5995    Img-5996.jpg\n",
      "5996    Img-5997.jpg\n",
      "5997    Img-5998.jpg\n",
      "5998    Img-5999.jpg\n",
      "5999    Img-6000.jpg\n",
      "Name: Image_id, Length: 6000, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>antelope</th>\n",
       "      <th>bat</th>\n",
       "      <th>beaver</th>\n",
       "      <th>bobcat</th>\n",
       "      <th>buffalo</th>\n",
       "      <th>chihuahua</th>\n",
       "      <th>chimpanzee</th>\n",
       "      <th>collie</th>\n",
       "      <th>dalmatian</th>\n",
       "      <th>...</th>\n",
       "      <th>raccoon</th>\n",
       "      <th>rat</th>\n",
       "      <th>rhinoceros</th>\n",
       "      <th>seal</th>\n",
       "      <th>siamese+cat</th>\n",
       "      <th>spider+monkey</th>\n",
       "      <th>squirrel</th>\n",
       "      <th>walrus</th>\n",
       "      <th>weasel</th>\n",
       "      <th>wolf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Img-1.jpg</td>\n",
       "      <td>1.497964e-08</td>\n",
       "      <td>1.949701e-08</td>\n",
       "      <td>1.632426e-08</td>\n",
       "      <td>2.997081e-07</td>\n",
       "      <td>9.981319e-01</td>\n",
       "      <td>1.231509e-14</td>\n",
       "      <td>1.183655e-06</td>\n",
       "      <td>1.668450e-08</td>\n",
       "      <td>6.104327e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>2.004127e-08</td>\n",
       "      <td>1.471354e-10</td>\n",
       "      <td>9.032629e-05</td>\n",
       "      <td>1.185133e-05</td>\n",
       "      <td>3.054609e-10</td>\n",
       "      <td>4.012723e-12</td>\n",
       "      <td>1.820806e-10</td>\n",
       "      <td>3.261427e-05</td>\n",
       "      <td>8.163424e-12</td>\n",
       "      <td>2.671979e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Img-2.jpg</td>\n",
       "      <td>5.120426e-09</td>\n",
       "      <td>3.373138e-06</td>\n",
       "      <td>1.421010e-05</td>\n",
       "      <td>3.276680e-06</td>\n",
       "      <td>3.929587e-08</td>\n",
       "      <td>2.162432e-03</td>\n",
       "      <td>2.851347e-05</td>\n",
       "      <td>1.032950e-09</td>\n",
       "      <td>3.383173e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.258814e-06</td>\n",
       "      <td>4.289428e-03</td>\n",
       "      <td>3.653901e-02</td>\n",
       "      <td>6.892457e-01</td>\n",
       "      <td>2.293787e-05</td>\n",
       "      <td>6.809890e-05</td>\n",
       "      <td>1.472451e-04</td>\n",
       "      <td>6.396946e-03</td>\n",
       "      <td>3.344103e-04</td>\n",
       "      <td>1.756603e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Img-3.jpg</td>\n",
       "      <td>9.928874e-01</td>\n",
       "      <td>1.992216e-05</td>\n",
       "      <td>1.503313e-04</td>\n",
       "      <td>1.374127e-06</td>\n",
       "      <td>1.694746e-05</td>\n",
       "      <td>1.291535e-03</td>\n",
       "      <td>1.519165e-07</td>\n",
       "      <td>1.276257e-05</td>\n",
       "      <td>2.091474e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.263892e-06</td>\n",
       "      <td>2.313075e-03</td>\n",
       "      <td>9.207237e-06</td>\n",
       "      <td>7.303223e-04</td>\n",
       "      <td>1.138934e-06</td>\n",
       "      <td>1.798488e-06</td>\n",
       "      <td>3.525436e-06</td>\n",
       "      <td>4.598647e-07</td>\n",
       "      <td>8.477054e-07</td>\n",
       "      <td>5.197485e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Img-4.jpg</td>\n",
       "      <td>1.369893e-10</td>\n",
       "      <td>1.904640e-04</td>\n",
       "      <td>3.816333e-09</td>\n",
       "      <td>1.398830e-06</td>\n",
       "      <td>3.481209e-18</td>\n",
       "      <td>9.695458e-01</td>\n",
       "      <td>3.086653e-06</td>\n",
       "      <td>7.569159e-08</td>\n",
       "      <td>1.470809e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>4.924225e-09</td>\n",
       "      <td>1.224925e-02</td>\n",
       "      <td>3.389440e-08</td>\n",
       "      <td>6.654133e-09</td>\n",
       "      <td>7.598905e-03</td>\n",
       "      <td>6.611165e-07</td>\n",
       "      <td>1.814470e-06</td>\n",
       "      <td>6.837266e-08</td>\n",
       "      <td>6.549297e-05</td>\n",
       "      <td>4.232138e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Img-5.jpg</td>\n",
       "      <td>8.211344e-12</td>\n",
       "      <td>3.340067e-02</td>\n",
       "      <td>1.864996e-08</td>\n",
       "      <td>2.914349e-05</td>\n",
       "      <td>2.445964e-09</td>\n",
       "      <td>2.676726e-03</td>\n",
       "      <td>7.929195e-11</td>\n",
       "      <td>7.087166e-10</td>\n",
       "      <td>1.098022e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274798e-06</td>\n",
       "      <td>2.702370e-03</td>\n",
       "      <td>1.483843e-06</td>\n",
       "      <td>4.859417e-06</td>\n",
       "      <td>9.286709e-01</td>\n",
       "      <td>1.410121e-05</td>\n",
       "      <td>2.454261e-02</td>\n",
       "      <td>1.100330e-05</td>\n",
       "      <td>5.236225e-06</td>\n",
       "      <td>1.364662e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Img-6.jpg</td>\n",
       "      <td>7.508039e-13</td>\n",
       "      <td>2.558778e-09</td>\n",
       "      <td>7.590010e-07</td>\n",
       "      <td>2.435752e-08</td>\n",
       "      <td>3.482711e-16</td>\n",
       "      <td>2.412254e-10</td>\n",
       "      <td>9.686543e-08</td>\n",
       "      <td>1.789664e-13</td>\n",
       "      <td>4.716123e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.393363e-08</td>\n",
       "      <td>4.761771e-09</td>\n",
       "      <td>1.263245e-11</td>\n",
       "      <td>9.999324e-01</td>\n",
       "      <td>6.322891e-11</td>\n",
       "      <td>1.635204e-06</td>\n",
       "      <td>6.982609e-09</td>\n",
       "      <td>5.605492e-05</td>\n",
       "      <td>1.138195e-06</td>\n",
       "      <td>4.842059e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Img-7.jpg</td>\n",
       "      <td>2.908742e-03</td>\n",
       "      <td>1.498656e-03</td>\n",
       "      <td>2.827088e-04</td>\n",
       "      <td>1.383671e-02</td>\n",
       "      <td>1.988077e-04</td>\n",
       "      <td>2.971498e-06</td>\n",
       "      <td>8.234620e-05</td>\n",
       "      <td>2.013518e-02</td>\n",
       "      <td>9.251644e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>4.716255e-04</td>\n",
       "      <td>6.133314e-05</td>\n",
       "      <td>3.423745e-05</td>\n",
       "      <td>1.244897e-05</td>\n",
       "      <td>5.734227e-06</td>\n",
       "      <td>2.272476e-04</td>\n",
       "      <td>5.528277e-04</td>\n",
       "      <td>4.736444e-05</td>\n",
       "      <td>6.895407e-02</td>\n",
       "      <td>1.430517e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Img-8.jpg</td>\n",
       "      <td>1.097310e-09</td>\n",
       "      <td>5.041008e-08</td>\n",
       "      <td>1.817613e-10</td>\n",
       "      <td>2.732491e-08</td>\n",
       "      <td>5.924206e-10</td>\n",
       "      <td>6.332332e-06</td>\n",
       "      <td>3.992504e-07</td>\n",
       "      <td>6.417870e-05</td>\n",
       "      <td>2.947899e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>6.921695e-08</td>\n",
       "      <td>3.402390e-07</td>\n",
       "      <td>4.490989e-10</td>\n",
       "      <td>5.041937e-07</td>\n",
       "      <td>2.049288e-07</td>\n",
       "      <td>9.910899e-08</td>\n",
       "      <td>2.662839e-10</td>\n",
       "      <td>1.715358e-09</td>\n",
       "      <td>9.265786e-08</td>\n",
       "      <td>4.991007e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Img-9.jpg</td>\n",
       "      <td>8.880340e-06</td>\n",
       "      <td>1.970157e-03</td>\n",
       "      <td>1.220477e-06</td>\n",
       "      <td>3.616779e-07</td>\n",
       "      <td>3.481840e-05</td>\n",
       "      <td>7.900029e-05</td>\n",
       "      <td>1.801909e-07</td>\n",
       "      <td>1.891657e-06</td>\n",
       "      <td>5.980168e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434130e-06</td>\n",
       "      <td>2.455263e-05</td>\n",
       "      <td>4.188533e-09</td>\n",
       "      <td>1.650384e-08</td>\n",
       "      <td>7.477143e-05</td>\n",
       "      <td>3.522885e-08</td>\n",
       "      <td>1.174824e-05</td>\n",
       "      <td>1.698952e-07</td>\n",
       "      <td>1.323324e-04</td>\n",
       "      <td>8.787772e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Img-10.jpg</td>\n",
       "      <td>1.121856e-09</td>\n",
       "      <td>8.563801e-11</td>\n",
       "      <td>1.642846e-08</td>\n",
       "      <td>2.129274e-06</td>\n",
       "      <td>2.055223e-17</td>\n",
       "      <td>2.936892e-03</td>\n",
       "      <td>5.171676e-08</td>\n",
       "      <td>5.284942e-02</td>\n",
       "      <td>9.349185e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.474701e-04</td>\n",
       "      <td>1.552514e-08</td>\n",
       "      <td>3.966292e-11</td>\n",
       "      <td>3.660392e-11</td>\n",
       "      <td>1.117981e-10</td>\n",
       "      <td>1.237980e-08</td>\n",
       "      <td>1.042077e-06</td>\n",
       "      <td>2.002563e-13</td>\n",
       "      <td>9.657186e-09</td>\n",
       "      <td>3.109099e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id      antelope           bat        beaver        bobcat  \\\n",
       "0   Img-1.jpg  1.497964e-08  1.949701e-08  1.632426e-08  2.997081e-07   \n",
       "1   Img-2.jpg  5.120426e-09  3.373138e-06  1.421010e-05  3.276680e-06   \n",
       "2   Img-3.jpg  9.928874e-01  1.992216e-05  1.503313e-04  1.374127e-06   \n",
       "3   Img-4.jpg  1.369893e-10  1.904640e-04  3.816333e-09  1.398830e-06   \n",
       "4   Img-5.jpg  8.211344e-12  3.340067e-02  1.864996e-08  2.914349e-05   \n",
       "5   Img-6.jpg  7.508039e-13  2.558778e-09  7.590010e-07  2.435752e-08   \n",
       "6   Img-7.jpg  2.908742e-03  1.498656e-03  2.827088e-04  1.383671e-02   \n",
       "7   Img-8.jpg  1.097310e-09  5.041008e-08  1.817613e-10  2.732491e-08   \n",
       "8   Img-9.jpg  8.880340e-06  1.970157e-03  1.220477e-06  3.616779e-07   \n",
       "9  Img-10.jpg  1.121856e-09  8.563801e-11  1.642846e-08  2.129274e-06   \n",
       "\n",
       "        buffalo     chihuahua    chimpanzee        collie     dalmatian  ...  \\\n",
       "0  9.981319e-01  1.231509e-14  1.183655e-06  1.668450e-08  6.104327e-14  ...   \n",
       "1  3.929587e-08  2.162432e-03  2.851347e-05  1.032950e-09  3.383173e-03  ...   \n",
       "2  1.694746e-05  1.291535e-03  1.519165e-07  1.276257e-05  2.091474e-06  ...   \n",
       "3  3.481209e-18  9.695458e-01  3.086653e-06  7.569159e-08  1.470809e-04  ...   \n",
       "4  2.445964e-09  2.676726e-03  7.929195e-11  7.087166e-10  1.098022e-06  ...   \n",
       "5  3.482711e-16  2.412254e-10  9.686543e-08  1.789664e-13  4.716123e-10  ...   \n",
       "6  1.988077e-04  2.971498e-06  8.234620e-05  2.013518e-02  9.251644e-03  ...   \n",
       "7  5.924206e-10  6.332332e-06  3.992504e-07  6.417870e-05  2.947899e-13  ...   \n",
       "8  3.481840e-05  7.900029e-05  1.801909e-07  1.891657e-06  5.980168e-09  ...   \n",
       "9  2.055223e-17  2.936892e-03  5.171676e-08  5.284942e-02  9.349185e-01  ...   \n",
       "\n",
       "        raccoon           rat    rhinoceros          seal   siamese+cat  \\\n",
       "0  2.004127e-08  1.471354e-10  9.032629e-05  1.185133e-05  3.054609e-10   \n",
       "1  3.258814e-06  4.289428e-03  3.653901e-02  6.892457e-01  2.293787e-05   \n",
       "2  1.263892e-06  2.313075e-03  9.207237e-06  7.303223e-04  1.138934e-06   \n",
       "3  4.924225e-09  1.224925e-02  3.389440e-08  6.654133e-09  7.598905e-03   \n",
       "4  1.274798e-06  2.702370e-03  1.483843e-06  4.859417e-06  9.286709e-01   \n",
       "5  3.393363e-08  4.761771e-09  1.263245e-11  9.999324e-01  6.322891e-11   \n",
       "6  4.716255e-04  6.133314e-05  3.423745e-05  1.244897e-05  5.734227e-06   \n",
       "7  6.921695e-08  3.402390e-07  4.490989e-10  5.041937e-07  2.049288e-07   \n",
       "8  5.434130e-06  2.455263e-05  4.188533e-09  1.650384e-08  7.477143e-05   \n",
       "9  1.474701e-04  1.552514e-08  3.966292e-11  3.660392e-11  1.117981e-10   \n",
       "\n",
       "   spider+monkey      squirrel        walrus        weasel          wolf  \n",
       "0   4.012723e-12  1.820806e-10  3.261427e-05  8.163424e-12  2.671979e-12  \n",
       "1   6.809890e-05  1.472451e-04  6.396946e-03  3.344103e-04  1.756603e-06  \n",
       "2   1.798488e-06  3.525436e-06  4.598647e-07  8.477054e-07  5.197485e-04  \n",
       "3   6.611165e-07  1.814470e-06  6.837266e-08  6.549297e-05  4.232138e-11  \n",
       "4   1.410121e-05  2.454261e-02  1.100330e-05  5.236225e-06  1.364662e-06  \n",
       "5   1.635204e-06  6.982609e-09  5.605492e-05  1.138195e-06  4.842059e-08  \n",
       "6   2.272476e-04  5.528277e-04  4.736444e-05  6.895407e-02  1.430517e-01  \n",
       "7   9.910899e-08  2.662839e-10  1.715358e-09  9.265786e-08  4.991007e-09  \n",
       "8   3.522885e-08  1.174824e-05  1.698952e-07  1.323324e-04  8.787772e-09  \n",
       "9   1.237980e-08  1.042077e-06  2.002563e-13  9.657186e-09  3.109099e-05  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classes = [\"antelope\",\"bat\",\"beaver\",\"bobcat\",\"buffalo\",\"chihuahua\",\"chimpanzee\",\"collie\",\"dalmatian\",\"german+shepherd\",\"grizzly+bear\",\n",
    "                \"hippopotamus\",\"horse\",\"killer+whale\",\"mole\",\"moose\",\"mouse\",\"otter\",\"ox\",\"persian+cat\",\"raccoon\",\"rat\",\"rhinoceros\",\"seal\",\n",
    "                \"siamese+cat\",\"spider+monkey\",\"squirrel\",\"walrus\",\"weasel\",\"wolf\"]\n",
    "label_df = pd.DataFrame(data=y_test_predict, columns= data_classes)\n",
    "\n",
    "label_df.head(10)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "\n",
    "\n",
    "te_label = pd.read_csv(path + '/test.csv')\n",
    "\n",
    "\n",
    "print(te_label['Image_id'])\n",
    "\n",
    "subm['image_id'] = te_label['Image_id']\n",
    "\n",
    "#print(subm.head(10))\n",
    "subm = pd.concat([subm, label_df], axis=1)\n",
    "\n",
    "subm.to_csv('submitvgg.csv',index = False)\n",
    "\n",
    "subm.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:47:56.196355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 11:47:57.042376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "# Load the ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Add a dense layer on top of the ResNet50 model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "predictions = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 70, 70, 3)    0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 32, 32, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 32, 32, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 32, 32, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 34, 34, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 16, 16, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 16, 16, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 16, 16, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 16, 16, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 8, 8, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 8, 8, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 8, 8, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 8, 8, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 4, 4, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 4, 4, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 2, 2, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 2, 2, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 2, 2, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 2, 2, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1024)         2098176     ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 30)           30750       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,716,638\n",
      "Trainable params: 25,663,518\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalyan/miniconda3/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "                loss='categorical_crossentropy',        \n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:50:05.667573: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n",
      "2023-04-08 11:50:06.111693: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 638976000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:50:11.224456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2023-04-08 11:50:13.184293: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 673.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.184344: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.503382: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.542747: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.542792: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.591556: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.591601: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.724953: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 673.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.743121: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 673.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-08 11:50:13.766501: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 36s 136ms/step - loss: 14.4476 - accuracy: 0.4005\n",
      "Epoch 2/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 10.3265 - accuracy: 0.7527\n",
      "Epoch 3/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 7.3759 - accuracy: 0.9195\n",
      "Epoch 4/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 5.2589 - accuracy: 0.9655\n",
      "Epoch 5/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 3.6922 - accuracy: 0.9768\n",
      "Epoch 6/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 2.5419 - accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 1.7838 - accuracy: 0.9665\n",
      "Epoch 8/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 1.2482 - accuracy: 0.9609\n",
      "Epoch 9/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 0.9171 - accuracy: 0.9548\n",
      "Epoch 10/10\n",
      "204/204 [==============================] - 27s 130ms/step - loss: 0.6224 - accuracy: 0.9712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9904f4070>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train/255., to_categorical(Y_train, num_classes=30), batch_size=64, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path + \"self_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 6s 22ms/step\n",
      "[[5.6558970e-06 4.6858077e-05 1.1881919e-02 ... 4.2412494e-04\n",
      "  7.0931906e-06 1.4301834e-06]\n",
      " [4.6921517e-03 3.3467944e-04 3.2227337e-03 ... 2.0731888e-03\n",
      "  1.3556154e-03 3.0760103e-04]\n",
      " [5.7905918e-01 1.1395101e-03 7.7154045e-03 ... 5.2771047e-03\n",
      "  1.3814449e-01 1.6585598e-04]\n",
      " ...\n",
      " [2.9387014e-05 5.5083847e-01 9.4704889e-03 ... 8.3074541e-05\n",
      "  6.5276829e-05 5.8108318e-04]\n",
      " [9.6382053e-07 1.7766629e-07 9.8776954e-06 ... 2.0847212e-08\n",
      "  9.4356061e-08 6.3748342e-07]\n",
      " [2.5379664e-04 7.2680326e-05 7.8604020e-02 ... 1.0547791e-04\n",
      "  1.4336433e-04 1.3985486e-04]]\n"
     ]
    }
   ],
   "source": [
    "#predicting on test data\n",
    "y_test_predict = model.predict(X_test/255.)\n",
    "print(y_test_predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
