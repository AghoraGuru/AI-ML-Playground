{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.117 🚀 Python-3.8.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msupersecurehuman\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/venom/repo/yolo_dish/wandb/run-20230613_104234-24sdufrs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/supersecurehuman/YOLOv8/runs/24sdufrs\" target=\"_blank\">denim-pine-2</a></strong> to <a href=\"https://wandb.ai/supersecurehuman/YOLOv8\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/venom/repo/yolo_dish/FinalDataset/labels/train.cache... 372 images, 0 backgrounds, 1 corrupt: 100%|██████████| 373/373 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/venom/repo/yolo_dish/FinalDataset/images/train/l63.jpg: ignoring corrupt image/label: image file is truncated (1 bytes not processed)\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/venom/repo/yolo_dish/FinalDataset/labels/val.cache... 47 images, 0 backgrounds, 0 corrupt: 100%|██████████| 47/47 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/venom/repo/yolo_dish/FinalDataset/images/val/517.jpg: 1 duplicate labels removed\n",
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      2.47G      1.044      4.193      1.142         82        640: 100%|██████████| 24/24 [00:09<00:00,  2.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]\n",
      "                   all         47        674     0.0136      0.012     0.0103    0.00981\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      2.47G     0.8557      3.519      1.024        109        640: 100%|██████████| 24/24 [00:02<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  8.09it/s]\n",
      "                   all         47        674       0.13      0.686      0.318      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10       2.5G     0.8533      2.753       1.04         77        640: 100%|██████████| 24/24 [00:02<00:00,  8.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "                   all         47        674      0.867      0.201      0.391      0.328\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10       2.5G     0.8599      2.088      1.065         87        640: 100%|██████████| 24/24 [00:02<00:00,  8.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n",
      "                   all         47        674      0.867      0.404      0.523      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      2.51G     0.8369      1.667      1.063        110        640: 100%|██████████| 24/24 [00:02<00:00,  8.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  6.77it/s]\n",
      "                   all         47        674      0.794      0.591      0.642      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      2.48G     0.8118      1.358      1.075        116        640: 100%|██████████| 24/24 [00:03<00:00,  7.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  5.99it/s]\n",
      "                   all         47        674      0.763      0.677      0.741      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      2.49G     0.7797      1.158      1.048         52        640: 100%|██████████| 24/24 [00:03<00:00,  7.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]\n",
      "                   all         47        674      0.862      0.754        0.8      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      2.48G     0.7653      1.018      1.048         94        640: 100%|██████████| 24/24 [00:03<00:00,  7.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  5.62it/s]\n",
      "                   all         47        674      0.863       0.82      0.828      0.697\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      2.33G     0.7486     0.9815      1.034         98        640: 100%|██████████| 24/24 [00:03<00:00,  7.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  5.63it/s]\n",
      "                   all         47        674      0.881      0.831      0.878      0.751\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      2.34G     0.7476     0.9353      1.031        125        640: 100%|██████████| 24/24 [00:03<00:00,  7.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "                   all         47        674      0.871      0.863      0.896      0.766\n",
      "\n",
      "10 epochs completed in 0.015 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.117 🚀 Python-3.8.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "Model summary (fused): 168 layers, 3009548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n",
      "                   all         47        674      0.869      0.865      0.897      0.766\n",
      "      Chopsticks-cover         47         11          1      0.196      0.672      0.639\n",
      "        Chopsticks-one         47         16       0.64      0.812      0.835      0.631\n",
      "        Chopsticks-two         47         46      0.834      0.957      0.955      0.803\n",
      "                Coffee         47         34      0.959          1      0.995      0.846\n",
      "            Coffee-cup         47          3      0.615          1      0.995      0.929\n",
      "           Coffee-dish         47          9      0.852      0.778      0.862      0.775\n",
      "                   Cup         47         70      0.916          1      0.962      0.839\n",
      "             Fish-dish         47         25      0.933          1      0.973      0.932\n",
      "                 Paper         47         10      0.824      0.942      0.978      0.879\n",
      "             Rice-bowl         47         39      0.975      0.949      0.956      0.854\n",
      "             Soup-bowl         47         38      0.908      0.974      0.965      0.841\n",
      "                 Spoon         47         41      0.813      0.951      0.953      0.768\n",
      "           Square-bowl         47         21      0.899          1      0.995      0.904\n",
      "               Tea-cup         47         95      0.831      0.968      0.944       0.78\n",
      "              Tea-dish         47          1          1          0      0.199      0.179\n",
      "                 Towel         47         47      0.834      0.979      0.953      0.735\n",
      "            Towel-dish         47         12      0.787      0.917      0.887      0.734\n",
      "          Waster-paper         47         30      0.859      0.933      0.927       0.68\n",
      "             Water-cup         47         60       0.92      0.983      0.949      0.775\n",
      "              Wine-cup         47         66      0.987      0.955      0.975      0.806\n",
      "Speed: 1.2ms preprocess, 1.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.train(data='data.yaml', epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
