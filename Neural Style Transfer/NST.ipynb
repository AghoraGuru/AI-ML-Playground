{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Style Transfer based on Neural Algorithm of Artistic Style paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to use the feature space from VGG Network and not using the last fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:43:48.919555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:43:50.444265: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64::/usr/local/tensorrt/lib/\n",
      "2023-03-02 17:43:50.444397: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64::/usr/local/tensorrt/lib/\n",
      "2023-03-02 17:43:50.444407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:44:02.132402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:02.354392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:02.354840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:02.355722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:44:02.357215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:02.357618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:02.357859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:03.688352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:03.689413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:03.689913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-02 17:44:03.690099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2564 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 181s 2us/step\n"
     ]
    }
   ],
   "source": [
    "#downloading the VGG19 model\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the content and style layers\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n",
    "\n",
    "#Defining the number of layers\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the content and style image\n",
    "content_image = tf.keras.preprocessing.image.load_img('content.jpg')\n",
    "style_image = tf.keras.preprocessing.image.load_img('style.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to load the VGG19 model\n",
    "def load_vgg_model():\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    #Getting the output of the content and style layers\n",
    "    content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "    style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "    #Getting the model\n",
    "    model_outputs = content_outputs + style_outputs\n",
    "    return tf.keras.Model(vgg.input, model_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the content and style features\n",
    "def get_content_and_style_features(model, content_image, style_image):\n",
    "    #Getting the content and style features\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs = model(style_image)\n",
    "    #Getting the content and style features\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[:num_content_layers]]\n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[num_content_layers:]]\n",
    "    return content_features, style_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the gram matrix\n",
    "def gram_matrix(input_tensor):\n",
    "    #Getting the shape of the tensor\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    #Getting the shape of the tensor\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    #Getting the gram matrix\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the style features\n",
    "def get_style_features(style_image):\n",
    "    #Getting the style features\n",
    "    style_features = gram_matrix(style_image)\n",
    "    return style_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the content features\n",
    "def get_content_features(content_image):\n",
    "    #Getting the content features\n",
    "    content_features = content_image\n",
    "    return content_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the loss\n",
    "def get_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    #Getting the style and content features\n",
    "    style_weight, content_weight = loss_weights\n",
    "    model_outputs = model(init_image)\n",
    "    #Getting the style and content features\n",
    "    style_output_features = model_outputs[num_content_layers:]\n",
    "    content_output_features = model_outputs[:num_content_layers]\n",
    "    #Getting the style and content loss\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "    #Getting the style and content loss\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        #Getting the style loss\n",
    "        style_score += weight_per_style_layer * tf.reduce_mean(tf.square(gram_matrix(comb_style[0]) - target_style))\n",
    "    #Getting the content loss\n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        #Getting the content loss\n",
    "        content_score += weight_per_content_layer*tf.reduce_mean(tf.square(comb_content[0] - target_content))\n",
    "    #Getting the total loss\n",
    "    style_score *= style_weight\n",
    "    content_score *= content_weight\n",
    "    #Getting the total loss\n",
    "    loss = style_score + content_score\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to get the gradients\n",
    "def get_grads(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    #Getting the gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Getting the loss\n",
    "        all_loss = get_loss(model, loss_weights, init_image, gram_style_features, content_features)\n",
    "    #Getting the gradients\n",
    "    total_loss = all_loss[0]\n",
    "    return tape.gradient(total_loss, init_image), all_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_img(processed_img):\n",
    "    #Getting the image\n",
    "    x = processed_img.copy()\n",
    "    #Getting the image\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "    #Getting the image\n",
    "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                               \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "    #Getting the image\n",
    "    if len(x.shape) != 3:\n",
    "        raise ValueError(\"Invalid input to deprocessing image\")\n",
    "    #Getting the image\n",
    "    if x.shape[2] == 1:\n",
    "        x = np.tile(x, (1, 1, 3))\n",
    "    #Getting the image\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    #Getting the image\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to run the style transfer\n",
    "def run_style_transfer(content_image, style_image, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
    "    #Loading the VGG19 model\n",
    "    model = load_vgg_model()\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    #Getting the content and style features\n",
    "    content_features, style_features = get_content_and_style_features(model, content_image, style_image)\n",
    "    #Getting the gram matrix\n",
    "    gram_style_features = [get_style_features(style_feature) for style_feature in style_features]\n",
    "    #Getting the content features\n",
    "    init_image = content_image\n",
    "    init_image = tf.Variable(init_image, dtype=tf.float32)\n",
    "    #Defining the optimizer\n",
    "    opt = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
    "    #Defining the best loss, best image and image\n",
    "    best_loss, best_img = float('inf'), None\n",
    "    #Defining the loss weights\n",
    "    loss_weights = (style_weight, content_weight)\n",
    "    #Defining the list of losses\n",
    "    iter_count = 1\n",
    "    #Defining the list of losses\n",
    "    losses = []\n",
    "    #Running the iterations\n",
    "    for i in range(num_iterations):\n",
    "        #Getting the gradients\n",
    "        grads, all_loss = get_grads(model, loss_weights, init_image, gram_style_features, content_features)\n",
    "        #Getting the loss, style loss and content loss\n",
    "        loss, style_score, content_score = all_loss\n",
    "        #Updating the variables\n",
    "        opt.apply_gradients([(grads, init_image)])\n",
    "        #Clipping the image\n",
    "        clipped = tf.clip_by_value(init_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "        init_image.assign(clipped)\n",
    "        #Updating the best loss, best image and image\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_img = deprocess_img(init_image.numpy())\n",
    "        #Printing the loss\n",
    "        if i % 100 == 0:\n",
    "            #Printing the loss\n",
    "            print ('Iteration: {}'.format(i))\n",
    "            print ('Total loss: {:.4e}, ' \n",
    "                   'style loss: {:.4e}, '\n",
    "                   'content loss: {:.4e}, '.format(loss, style_score, content_score))\n",
    "            print()\n",
    "        #Appending the loss\n",
    "        losses.append(loss)\n",
    "    #Returning the best loss, best image and losses\n",
    "    return best_img, best_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    #Defining the image size\n",
    "    img_size = 512\n",
    "    #Resizing the content and style image\n",
    "    content_image = tf.keras.preprocessing.image.load_img('content.jpeg', target_size=(img_size, img_size))\n",
    "    style_image = tf.keras.preprocessing.image.load_img('style.jpeg', target_size=(img_size, img_size))\n",
    "    #Converting the image to array\n",
    "    content_image = tf.keras.preprocessing.image.img_to_array(content_image)\n",
    "    style_image = tf.keras.preprocessing.image.img_to_array(style_image)\n",
    "    #Converting the image to float32\n",
    "    content_image = tf.cast(content_image, tf.float32)\n",
    "    style_image = tf.cast(style_image, tf.float32)\n",
    "    #Normalizing the image\n",
    "    content_image = content_image/255.0\n",
    "    style_image = style_image/255.0\n",
    "    #Reshaping the image\n",
    "    content_image = content_image[tf.newaxis, :]\n",
    "    style_image = style_image[tf.newaxis, :]\n",
    "\n",
    "    return content_image, style_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(1, 512, 512, 3), dtype=float32, numpy=\narray([[[[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196]],\n\n        [[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196]],\n\n        [[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.6392157 , 0.72156864, 0.45882353],\n         [0.6392157 , 0.72156864, 0.45882353],\n         [0.6392157 , 0.72156864, 0.45882353]],\n\n        ...,\n\n        [[0.39215687, 0.47843137, 0.22745098],\n         [0.38431373, 0.47058824, 0.21960784],\n         [0.38431373, 0.47058824, 0.22745098],\n         ...,\n         [0.47058824, 0.49019608, 0.2784314 ],\n         [0.46666667, 0.47843137, 0.27058825],\n         [0.47058824, 0.48235294, 0.27450982]],\n\n        [[0.39215687, 0.47843137, 0.22352941],\n         [0.3882353 , 0.4745098 , 0.21960784],\n         [0.3882353 , 0.4745098 , 0.22352941],\n         ...,\n         [0.4627451 , 0.49019608, 0.2784314 ],\n         [0.47058824, 0.49019608, 0.2784314 ],\n         [0.4745098 , 0.4862745 , 0.2784314 ]],\n\n        [[0.3882353 , 0.47843137, 0.21176471],\n         [0.3882353 , 0.47843137, 0.21176471],\n         [0.38431373, 0.47058824, 0.21568628],\n         ...,\n         [0.4627451 , 0.49803922, 0.2901961 ],\n         [0.47058824, 0.49803922, 0.29411766],\n         [0.4745098 , 0.49019608, 0.2901961 ]]]], dtype=float32)>, <tf.Tensor: shape=(1, 512, 512, 3), dtype=float32, numpy=\narray([[[[0.13725491, 0.13725491, 0.17254902],\n         [0.09803922, 0.11372549, 0.1882353 ],\n         [0.12941177, 0.15686275, 0.25490198],\n         ...,\n         [0.6509804 , 0.58431375, 0.44705883],\n         [0.7607843 , 0.69411767, 0.5568628 ],\n         [0.7254902 , 0.65882355, 0.52156866]],\n\n        [[0.11764706, 0.11764706, 0.18039216],\n         [0.1254902 , 0.13725491, 0.24705882],\n         [0.11764706, 0.14117648, 0.2784314 ],\n         ...,\n         [0.73333335, 0.6666667 , 0.5294118 ],\n         [0.67058825, 0.6039216 , 0.46666667],\n         [0.75686276, 0.6901961 , 0.5529412 ]],\n\n        [[0.14117648, 0.13333334, 0.22352941],\n         [0.1254902 , 0.12941177, 0.27450982],\n         [0.14509805, 0.16470589, 0.32941177],\n         ...,\n         [0.8392157 , 0.77254903, 0.63529414],\n         [0.81960785, 0.7529412 , 0.6156863 ],\n         [0.6       , 0.53333336, 0.39607844]],\n\n        ...,\n\n        [[0.54901963, 0.44705883, 0.29411766],\n         [0.49019608, 0.42352942, 0.29411766],\n         [0.7254902 , 0.6901961 , 0.5882353 ],\n         ...,\n         [0.2901961 , 0.25490198, 0.20392157],\n         [0.4117647 , 0.36862746, 0.26666668],\n         [0.56078434, 0.5137255 , 0.36078432]],\n\n        [[0.53333336, 0.46666667, 0.32156864],\n         [0.654902  , 0.59607846, 0.4627451 ],\n         [0.5019608 , 0.44705883, 0.29803923],\n         ...,\n         [0.34509805, 0.30588236, 0.20392157],\n         [0.67058825, 0.61960787, 0.49411765],\n         [0.6392157 , 0.58431375, 0.43529412]],\n\n        [[0.5254902 , 0.47058824, 0.32156864],\n         [0.47843137, 0.42352942, 0.31764707],\n         [0.5568628 , 0.49411765, 0.39215687],\n         ...,\n         [0.7411765 , 0.6901961 , 0.5647059 ],\n         [0.70980394, 0.6431373 , 0.5137255 ],\n         [0.63529414, 0.5568628 , 0.42352942]]]], dtype=float32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m style_image \u001b[39m=\u001b[39m load_img(style_path)\n\u001b[1;32m      8\u001b[0m \u001b[39m#Running the style transfer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m best, best_loss, losses \u001b[39m=\u001b[39m run_style_transfer(content_image, style_image, num_iterations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [17], line 8\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[0;34m(content_image, style_image, num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m      6\u001b[0m     layer\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#Getting the content and style features\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m content_features, style_features \u001b[39m=\u001b[39m get_content_and_style_features(model, content_image, style_image)\n\u001b[1;32m      9\u001b[0m \u001b[39m#Getting the gram matrix\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gram_style_features \u001b[39m=\u001b[39m [get_style_features(style_feature) \u001b[39mfor\u001b[39;00m style_feature \u001b[39min\u001b[39;00m style_features]\n",
      "Cell \u001b[0;32mIn [10], line 4\u001b[0m, in \u001b[0;36mget_content_and_style_features\u001b[0;34m(model, content_image, style_image)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_content_and_style_features\u001b[39m(model, content_image, style_image):\n\u001b[1;32m      3\u001b[0m     \u001b[39m#Getting the content and style features\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     content_outputs \u001b[39m=\u001b[39m model(content_image)\n\u001b[1;32m      5\u001b[0m     style_outputs \u001b[39m=\u001b[39m model(style_image)\n\u001b[1;32m      6\u001b[0m     \u001b[39m#Getting the content and style features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/keras/engine/input_spec.py:216\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs to a layer should be tensors. Got: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(input_spec):\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m expects \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_spec)\u001b[39m}\u001b[39;00m\u001b[39m input(s),\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but it received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m input tensors. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs received: \u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    221\u001b[0m \u001b[39mfor\u001b[39;00m input_index, (x, spec) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(inputs, input_spec)):\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(1, 512, 512, 3), dtype=float32, numpy=\narray([[[[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196]],\n\n        [[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196],\n         [0.63529414, 0.7176471 , 0.45490196]],\n\n        [[0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         [0.6509804 , 0.7372549 , 0.48235294],\n         ...,\n         [0.6392157 , 0.72156864, 0.45882353],\n         [0.6392157 , 0.72156864, 0.45882353],\n         [0.6392157 , 0.72156864, 0.45882353]],\n\n        ...,\n\n        [[0.39215687, 0.47843137, 0.22745098],\n         [0.38431373, 0.47058824, 0.21960784],\n         [0.38431373, 0.47058824, 0.22745098],\n         ...,\n         [0.47058824, 0.49019608, 0.2784314 ],\n         [0.46666667, 0.47843137, 0.27058825],\n         [0.47058824, 0.48235294, 0.27450982]],\n\n        [[0.39215687, 0.47843137, 0.22352941],\n         [0.3882353 , 0.4745098 , 0.21960784],\n         [0.3882353 , 0.4745098 , 0.22352941],\n         ...,\n         [0.4627451 , 0.49019608, 0.2784314 ],\n         [0.47058824, 0.49019608, 0.2784314 ],\n         [0.4745098 , 0.4862745 , 0.2784314 ]],\n\n        [[0.3882353 , 0.47843137, 0.21176471],\n         [0.3882353 , 0.47843137, 0.21176471],\n         [0.38431373, 0.47058824, 0.21568628],\n         ...,\n         [0.4627451 , 0.49803922, 0.2901961 ],\n         [0.47058824, 0.49803922, 0.29411766],\n         [0.4745098 , 0.49019608, 0.2901961 ]]]], dtype=float32)>, <tf.Tensor: shape=(1, 512, 512, 3), dtype=float32, numpy=\narray([[[[0.13725491, 0.13725491, 0.17254902],\n         [0.09803922, 0.11372549, 0.1882353 ],\n         [0.12941177, 0.15686275, 0.25490198],\n         ...,\n         [0.6509804 , 0.58431375, 0.44705883],\n         [0.7607843 , 0.69411767, 0.5568628 ],\n         [0.7254902 , 0.65882355, 0.52156866]],\n\n        [[0.11764706, 0.11764706, 0.18039216],\n         [0.1254902 , 0.13725491, 0.24705882],\n         [0.11764706, 0.14117648, 0.2784314 ],\n         ...,\n         [0.73333335, 0.6666667 , 0.5294118 ],\n         [0.67058825, 0.6039216 , 0.46666667],\n         [0.75686276, 0.6901961 , 0.5529412 ]],\n\n        [[0.14117648, 0.13333334, 0.22352941],\n         [0.1254902 , 0.12941177, 0.27450982],\n         [0.14509805, 0.16470589, 0.32941177],\n         ...,\n         [0.8392157 , 0.77254903, 0.63529414],\n         [0.81960785, 0.7529412 , 0.6156863 ],\n         [0.6       , 0.53333336, 0.39607844]],\n\n        ...,\n\n        [[0.54901963, 0.44705883, 0.29411766],\n         [0.49019608, 0.42352942, 0.29411766],\n         [0.7254902 , 0.6901961 , 0.5882353 ],\n         ...,\n         [0.2901961 , 0.25490198, 0.20392157],\n         [0.4117647 , 0.36862746, 0.26666668],\n         [0.56078434, 0.5137255 , 0.36078432]],\n\n        [[0.53333336, 0.46666667, 0.32156864],\n         [0.654902  , 0.59607846, 0.4627451 ],\n         [0.5019608 , 0.44705883, 0.29803923],\n         ...,\n         [0.34509805, 0.30588236, 0.20392157],\n         [0.67058825, 0.61960787, 0.49411765],\n         [0.6392157 , 0.58431375, 0.43529412]],\n\n        [[0.5254902 , 0.47058824, 0.32156864],\n         [0.47843137, 0.42352942, 0.31764707],\n         [0.5568628 , 0.49411765, 0.39215687],\n         ...,\n         [0.7411765 , 0.6901961 , 0.5647059 ],\n         [0.70980394, 0.6431373 , 0.5137255 ],\n         [0.63529414, 0.5568628 , 0.42352942]]]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "content_path = 'content.jpeg'\n",
    "style_path = 'style.jpeg'\n",
    "\n",
    "#Loading the content image\n",
    "content_image = load_img(content_path)\n",
    "#Loading the style image\n",
    "style_image = load_img(style_path)\n",
    "#Running the style transfer\n",
    "best, best_loss, losses = run_style_transfer(content_image, style_image, num_iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "858efc061b545416e25caa8d4697e72d127b15a1bac3735708778cd5f37dd3d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
